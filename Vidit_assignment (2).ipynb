{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zooqWs-z2TtO",
        "outputId": "94df65a5-3816-4b26-ac92-823e9325eabe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 pandas openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Define the list of companies with their websites\n",
        "companies = [\n",
        "    {\"id\": 5875, \"name\": \"Solarkal\", \"website\": \"https://www.solarkal.com/\"},\n",
        "    {\"id\": 11917, \"name\": \"H2Scan\", \"website\": \"https://h2scan.com/\"},\n",
        "    {\"id\": 34005, \"name\": \"Eo Charging\", \"website\": \"https://www.eocharging.com/\"},\n",
        "    {\"id\": 65212, \"name\": \"Prewave\", \"website\": \"https://www.prewave.com/\"},\n",
        "    {\"id\": 18533, \"name\": \"Viriciti\", \"website\": \"https://www.chargepoint.com/\"},\n",
        "    {\"id\": 2805, \"name\": \"EasyMile\", \"website\": \"https://www.easymile.com/\"},\n",
        "    {\"id\": 101741, \"name\": \"Everstream\", \"website\": \"https://www.everstream.ai/\"},\n",
        "    {\"id\": 110133, \"name\": \"Altus Power\", \"website\": \"https://www.altuspower.com/\"},\n",
        "    {\"id\": 12605, \"name\": \"Charm Industrial\", \"website\": \"https://www.charmindustrial.com/\"},\n",
        "    {\"id\": 105894, \"name\": \"Isotropic Systems\", \"website\": \"https://www.all.space/\"},\n",
        "    {\"id\": 400, \"name\": \"Caban Systems\", \"website\": \"https://www.cabanenergy.com/\"},\n",
        "    {\"id\": 34204, \"name\": \"BioBTX\", \"website\": \"https://biobtx.com/\"},\n",
        "    {\"id\": 6134, \"name\": \"Hydrogenious LOHC\", \"website\": \"https://hydrogenious.net/\"},\n",
        "    {\"id\": 12008, \"name\": \"Iogen\", \"website\": \"https://www.iogen.com/\"},\n",
        "    {\"id\": 6997, \"name\": \"Infinited Fiber Company\", \"website\": \"https://www.infinitedfiber.com/\"}\n",
        "]\n",
        "\n",
        "# Function to scrape data for a company\n",
        "def scrape_company_data(company):\n",
        "    try:\n",
        "        response = requests.get(company['website'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract description\n",
        "        description = soup.find('meta', attrs={'name': 'description'})['content'] if soup.find('meta', attrs={'name': 'description'}) else \"No description available\"\n",
        "\n",
        "        # Placeholder data for HQ and Offices, Clients, and News (to be implemented as per website structure)\n",
        "        hq_offices = \"HQ and Offices details to be implemented\"\n",
        "        clients = \"Clients details to be implemented\"\n",
        "        news = \"News details to be implemented\"\n",
        "\n",
        "        return {\n",
        "            \"Company ID\": company['id'],\n",
        "            \"Company Name\": company['name'],\n",
        "            \"Website\": company['website'],\n",
        "            \"Description\": description,\n",
        "            \"HQ and Offices\": hq_offices,\n",
        "            \"Clients\": clients,\n",
        "            \"News\": news\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {company['name']}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Scrape data for all companies\n",
        "data = []\n",
        "for company in companies:\n",
        "    company_data = scrape_company_data(company)\n",
        "    if company_data:\n",
        "        data.append(company_data)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save DataFrame to Excel mimicking SQL table structure\n",
        "df.to_excel('company_data.xlsx', index=False)\n",
        "\n",
        "print(\"Data scraping complete. The results have been saved to 'company_data.xlsx'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WpLg24Q2csi",
        "outputId": "566ac694-b445-4f8f-b3f5-d29dc197c6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data scraping complete. The results have been saved to 'company_data.xlsx'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('company_data.xlsx')\n",
        "\n",
        "# Display the contents\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSt7NA_R2spx",
        "outputId": "b36fdf01-2826-474d-b4b2-5509f3b0eafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Company ID             Company Name                           Website  \\\n",
            "0         5875                 Solarkal         https://www.solarkal.com/   \n",
            "1        11917                   H2Scan               https://h2scan.com/   \n",
            "2        34005              Eo Charging       https://www.eocharging.com/   \n",
            "3        65212                  Prewave          https://www.prewave.com/   \n",
            "4        18533                 Viriciti      https://www.chargepoint.com/   \n",
            "5         2805                 EasyMile         https://www.easymile.com/   \n",
            "6       101741               Everstream        https://www.everstream.ai/   \n",
            "7       110133              Altus Power       https://www.altuspower.com/   \n",
            "8        12605         Charm Industrial  https://www.charmindustrial.com/   \n",
            "9       105894        Isotropic Systems            https://www.all.space/   \n",
            "10         400            Caban Systems      https://www.cabanenergy.com/   \n",
            "11       34204                   BioBTX               https://biobtx.com/   \n",
            "12        6134        Hydrogenious LOHC         https://hydrogenious.net/   \n",
            "13       12008                    Iogen            https://www.iogen.com/   \n",
            "14        6997  Infinited Fiber Company   https://www.infinitedfiber.com/   \n",
            "\n",
            "                                          Description  \\\n",
            "0   SolarKal is the leading commercial solar advis...   \n",
            "1   H2scan’s proven sensing technology, based on R...   \n",
            "2   Our commercial EV charging infrastructure solu...   \n",
            "3   Supplier monitoring for purchasing, supply cha...   \n",
            "4                            No description available   \n",
            "5   Driverless vehicle solutions and full-service ...   \n",
            "6                            No description available   \n",
            "7   Take control of your sustainability and decarb...   \n",
            "8   Charm Industrial provides high-quality carbon ...   \n",
            "9   ALL.SPACE is revolutionising communications wi...   \n",
            "10  Reimagining how we power the planet. Energy st...   \n",
            "11  We at BioBTX developed a technology to produce...   \n",
            "12  We store and transport hydrogen in a liquid or...   \n",
            "13  Iogen carbon-negative fuel production technolo...   \n",
            "14  Let's make textile circularity an everyday rea...   \n",
            "\n",
            "                              HQ and Offices  \\\n",
            "0   HQ and Offices details to be implemented   \n",
            "1   HQ and Offices details to be implemented   \n",
            "2   HQ and Offices details to be implemented   \n",
            "3   HQ and Offices details to be implemented   \n",
            "4   HQ and Offices details to be implemented   \n",
            "5   HQ and Offices details to be implemented   \n",
            "6   HQ and Offices details to be implemented   \n",
            "7   HQ and Offices details to be implemented   \n",
            "8   HQ and Offices details to be implemented   \n",
            "9   HQ and Offices details to be implemented   \n",
            "10  HQ and Offices details to be implemented   \n",
            "11  HQ and Offices details to be implemented   \n",
            "12  HQ and Offices details to be implemented   \n",
            "13  HQ and Offices details to be implemented   \n",
            "14  HQ and Offices details to be implemented   \n",
            "\n",
            "                              Clients                            News  \n",
            "0   Clients details to be implemented  News details to be implemented  \n",
            "1   Clients details to be implemented  News details to be implemented  \n",
            "2   Clients details to be implemented  News details to be implemented  \n",
            "3   Clients details to be implemented  News details to be implemented  \n",
            "4   Clients details to be implemented  News details to be implemented  \n",
            "5   Clients details to be implemented  News details to be implemented  \n",
            "6   Clients details to be implemented  News details to be implemented  \n",
            "7   Clients details to be implemented  News details to be implemented  \n",
            "8   Clients details to be implemented  News details to be implemented  \n",
            "9   Clients details to be implemented  News details to be implemented  \n",
            "10  Clients details to be implemented  News details to be implemented  \n",
            "11  Clients details to be implemented  News details to be implemented  \n",
            "12  Clients details to be implemented  News details to be implemented  \n",
            "13  Clients details to be implemented  News details to be implemented  \n",
            "14  Clients details to be implemented  News details to be implemented  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the file\n",
        "files.download('company_data.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "g_fP0K382mY3",
        "outputId": "3353015a-6b08-4746-ac61-d3be91d30965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_77a90a8d-2b25-4a62-901a-89d59fc4b735\", \"company_data.xlsx\", 7055)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding\n",
        "\n",
        "# 1) Description Extraction\n",
        "\n",
        "# 2)HQ and Offices Extraction\n",
        "\n",
        "# 3) News Extraction\n",
        "\n",
        "# 4) Clients Extraction"
      ],
      "metadata": {
        "id": "DSyYqpt_3S7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Define the list of companies with their websites\n",
        "companies = [\n",
        "    {\"id\": 5875, \"name\": \"Solarkal\", \"website\": \"https://www.solarkal.com/\"},\n",
        "    {\"id\": 11917, \"name\": \"H2Scan\", \"website\": \"https://h2scan.com/\"},\n",
        "    {\"id\": 34005, \"name\": \"Eo Charging\", \"website\": \"https://www.eocharging.com/\"},\n",
        "    {\"id\": 65212, \"name\": \"Prewave\", \"website\": \"https://www.prewave.com/\"},\n",
        "    {\"id\": 18533, \"name\": \"Viriciti\", \"website\": \"https://www.chargepoint.com/\"},\n",
        "    {\"id\": 2805, \"name\": \"EasyMile\", \"website\": \"https://www.easymile.com/\"},\n",
        "    {\"id\": 101741, \"name\": \"Everstream\", \"website\": \"https://www.everstream.ai/\"},\n",
        "    {\"id\": 110133, \"name\": \"Altus Power\", \"website\": \"https://www.altuspower.com/\"},\n",
        "    {\"id\": 12605, \"name\": \"Charm Industrial\", \"website\": \"https://www.charmindustrial.com/\"},\n",
        "    {\"id\": 105894, \"name\": \"Isotropic Systems\", \"website\": \"https://www.all.space/\"},\n",
        "    {\"id\": 400, \"name\": \"Caban Systems\", \"website\": \"https://www.cabanenergy.com/\"},\n",
        "    {\"id\": 34204, \"name\": \"BioBTX\", \"website\": \"https://biobtx.com/\"},\n",
        "    {\"id\": 6134, \"name\": \"Hydrogenious LOHC\", \"website\": \"https://hydrogenious.net/\"},\n",
        "    {\"id\": 12008, \"name\": \"Iogen\", \"website\": \"https://www.iogen.com/\"},\n",
        "    {\"id\": 6997, \"name\": \"Infinited Fiber Company\", \"website\": \"https://www.infinitedfiber.com/\"}\n",
        "]\n",
        "\n",
        "# Function to extract company description\n",
        "def get_description(soup):\n",
        "    description = soup.find('meta', attrs={'name': 'description'})\n",
        "    if description:\n",
        "        return description.get('content')\n",
        "    fallback_description = soup.find('section', {'class': 'description'}) or soup.find('div', {'class': 'about-us'})\n",
        "    return fallback_description.get_text(strip=True) if fallback_description else \"No description available\"\n",
        "\n",
        "# Function to extract HQ and office locations\n",
        "def get_hq_offices(soup):\n",
        "    offices_section = soup.find('div', {'class': 'office-locations'}) or \\\n",
        "                      soup.find('section', {'id': 'contact'}) or \\\n",
        "                      soup.find(string=lambda t: \"office\" in t.lower())\n",
        "    if offices_section:\n",
        "        if isinstance(offices_section, str):\n",
        "            return offices_section.strip()\n",
        "        offices = [office.get_text(strip=True) for office in offices_section.find_all('li')]\n",
        "        return ', '.join(offices) if offices else offices_section.get_text(strip=True)\n",
        "    return \"No offices found\"\n",
        "\n",
        "# Function to extract clients\n",
        "def get_clients(soup):\n",
        "    clients_section = soup.find('div', {'class': 'client-logos'}) or \\\n",
        "                      soup.find('section', {'id': 'clients'}) or \\\n",
        "                      soup.find_all('img', alt=True)\n",
        "    if clients_section:\n",
        "        if isinstance(clients_section, list):\n",
        "            clients = [client.get('alt') for client in clients_section if client.get('alt')]\n",
        "        else:\n",
        "            clients = [client.get('alt') for client in clients_section.find_all('img')]\n",
        "        return ', '.join(clients) if clients else \"No clients found\"\n",
        "    return \"No clients found\"\n",
        "\n",
        "# Function to extract latest news\n",
        "def get_news(soup):\n",
        "    news_list = []\n",
        "    news_section = soup.find('section', {'id': 'news'}) or \\\n",
        "                   soup.find('div', {'class': 'latest-news'}) or \\\n",
        "                   soup.find_all('article')\n",
        "    if news_section:\n",
        "        articles = news_section.find_all('article') if hasattr(news_section, 'find_all') else news_section\n",
        "        for article in articles:\n",
        "            title = article.find('h2').get_text(strip=True) if article.find('h2') else \"No title\"\n",
        "            date = article.find('time').get_text(strip=True) if article.find('time') else \"No date\"\n",
        "            url = article.find('a').get('href') if article.find('a') else \"No URL\"\n",
        "            summary = article.find('p').get_text(strip=True) if article.find('p') else \"No summary\"\n",
        "            news_list.append(f\"Title: {title}, Date: {date}, URL: {url}, Summary: {summary}\")\n",
        "    return ' | '.join(news_list) if news_list else \"No news found\"\n",
        "\n",
        "# Function to scrape data for a company\n",
        "def scrape_company_data(company):\n",
        "    try:\n",
        "        response = requests.get(company['website'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract data\n",
        "        description = get_description(soup)\n",
        "        hq_offices = get_hq_offices(soup)\n",
        "        clients = get_clients(soup)\n",
        "        news = get_news(soup)\n",
        "\n",
        "        return {\n",
        "            \"Company ID\": company['id'],\n",
        "            \"Company Name\": company['name'],\n",
        "            \"Website\": company['website'],\n",
        "            \"Description\": description,\n",
        "            \"HQ and Offices\": hq_offices,\n",
        "            \"Clients\": clients,\n",
        "            \"News\": news\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {company['name']}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Scrape data for all companies\n",
        "data = []\n",
        "for company in companies:\n",
        "    company_data = scrape_company_data(company)\n",
        "    if company_data:\n",
        "        data.append(company_data)\n",
        "\n",
        "# Create a DataFrame\n",
        "df2 = pd.DataFrame(data)\n",
        "\n",
        "# Save DataFrame to Excel mimicking SQL table structure\n",
        "df2.to_excel('company_data_enhanced.xlsx', index=False)\n",
        "\n",
        "print(\"Enhanced data scraping complete. The results have been saved to 'company_data_enhanced.xlsx'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKqeTvWcAQxD",
        "outputId": "630d9a6b-c261-4069-ce38-e1ffec2ab23c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced data scraping complete. The results have been saved to 'company_data_enhanced.xlsx'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Excel file\n",
        "df2 = pd.read_excel('company_data_enhanced.xlsx')\n",
        "\n",
        "# Display the contents\n",
        "print(df2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZMx_X1i_IpG",
        "outputId": "b9140f54-6928-4261-b15a-f1cd3734ca22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Company ID             Company Name                           Website  \\\n",
            "0         5875                 Solarkal         https://www.solarkal.com/   \n",
            "1        11917                   H2Scan               https://h2scan.com/   \n",
            "2        34005              Eo Charging       https://www.eocharging.com/   \n",
            "3        65212                  Prewave          https://www.prewave.com/   \n",
            "4        18533                 Viriciti      https://www.chargepoint.com/   \n",
            "5         2805                 EasyMile         https://www.easymile.com/   \n",
            "6       101741               Everstream        https://www.everstream.ai/   \n",
            "7       110133              Altus Power       https://www.altuspower.com/   \n",
            "8        12605         Charm Industrial  https://www.charmindustrial.com/   \n",
            "9       105894        Isotropic Systems            https://www.all.space/   \n",
            "10         400            Caban Systems      https://www.cabanenergy.com/   \n",
            "11       34204                   BioBTX               https://biobtx.com/   \n",
            "12        6134        Hydrogenious LOHC         https://hydrogenious.net/   \n",
            "13       12008                    Iogen            https://www.iogen.com/   \n",
            "14        6997  Infinited Fiber Company   https://www.infinitedfiber.com/   \n",
            "\n",
            "                                          Description  \\\n",
            "0   SolarKal is the leading commercial solar advis...   \n",
            "1   H2scan’s proven sensing technology, based on R...   \n",
            "2   Our commercial EV charging infrastructure solu...   \n",
            "3   Supplier monitoring for purchasing, supply cha...   \n",
            "4                            No description available   \n",
            "5   Driverless vehicle solutions and full-service ...   \n",
            "6                            No description available   \n",
            "7   Take control of your sustainability and decarb...   \n",
            "8   Charm Industrial provides high-quality carbon ...   \n",
            "9   ALL.SPACE is revolutionising communications wi...   \n",
            "10  Reimagining how we power the planet. Energy st...   \n",
            "11  We at BioBTX developed a technology to produce...   \n",
            "12  We store and transport hydrogen in a liquid or...   \n",
            "13  Iogen carbon-negative fuel production technolo...   \n",
            "14  Let's make textile circularity an everyday rea...   \n",
            "\n",
            "                            HQ and Offices  \\\n",
            "0   Director, Campus Sustainability Office   \n",
            "1                         No offices found   \n",
            "2                      3 Offices Worldwide   \n",
            "3                         No offices found   \n",
            "4                         No offices found   \n",
            "5                         No offices found   \n",
            "6                         No offices found   \n",
            "7                         No offices found   \n",
            "8                         No offices found   \n",
            "9                         No offices found   \n",
            "10                        No offices found   \n",
            "11                        No offices found   \n",
            "12                        No offices found   \n",
            "13                        No offices found   \n",
            "14                        No offices found   \n",
            "\n",
            "                                              Clients  \\\n",
            "0                                    No clients found   \n",
            "1   abb, alabamapower, blueorigin, boeing, bp, con...   \n",
            "2   EO Genius, London Bus charging, EO Cloud Lapto...   \n",
            "3   ClickCease, Mercedes-benz-logo, Audi_logo.svg,...   \n",
            "4   Home, Home, Careers-nav-thumb, Laptop displayi...   \n",
            "5   EZTow autonomous tow tractor, Autonomous Termi...   \n",
            "6                                    No clients found   \n",
            "7   Close Icon, Birds eye view of commercial rooft...   \n",
            "8   Charm Industrial Homepage, Frontier, Stripe, A...   \n",
            "9                                    No clients found   \n",
            "10  Caban's hardware, Enduro and Monaco, and softw...   \n",
            "11                                   No clients found   \n",
            "12  Hydrogenious LOHC tanker truck, Hydrogenious L...   \n",
            "13  Iogen Icons Clean Fuel, Iogen Icons Net Zero, ...   \n",
            "14  Infinited Fiber, Infinited Fiber, Infinited Fiber   \n",
            "\n",
            "                                                 News  \n",
            "0                                       No news found  \n",
            "1   Title: No title, Date: No date, URL: https://h...  \n",
            "2                                       No news found  \n",
            "3                                       No news found  \n",
            "4   Title: Connect with drivers when they want to ...  \n",
            "5   Title: There Is Money To Be Made In Driverless...  \n",
            "6                                       No news found  \n",
            "7                                       No news found  \n",
            "8   Title: No title, Date: No date, URL: No URL, S...  \n",
            "9                                       No news found  \n",
            "10                                      No news found  \n",
            "11                                      No news found  \n",
            "12  Title: No title, Date: No date, URL: /what/#st...  \n",
            "13                                      No news found  \n",
            "14  Title: Introducing Infinna™– the circular fibe...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the file\n",
        "files.download('company_data_enhanced.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "kU7UybsTBAd3",
        "outputId": "6ec3fcfa-c153-4c27-c7dc-3df7adc836c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f8e8adc9-3b8d-4317-b61b-8ffab81c6794\", \"company_data_enhanced.xlsx\", 9894)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GenAi"
      ],
      "metadata": {
        "id": "OYehxxy_BRG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVP02ZfrBre7",
        "outputId": "ff92d59c-ce3a-4d78-d3ae-1e18f1aabde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 openai-1.42.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "K0vBkTxOB9LH",
        "outputId": "5c341cb6-b7c5-461d-ed37-3bc198191c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.42.0\n",
            "    Uninstalling openai-1.42.0:\n",
            "      Successfully uninstalled openai-1.42.0\n",
            "Successfully installed openai-0.28.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              },
              "id": "9b5df96388ee4f318bc7872b29783318"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Define the list of companies with their websites\n",
        "companies = [\n",
        "    {\"id\": 5875, \"name\": \"Solarkal\", \"website\": \"https://www.solarkal.com/\"},\n",
        "    {\"id\": 11917, \"name\": \"H2Scan\", \"website\": \"https://h2scan.com/\"},\n",
        "    {\"id\": 34005, \"name\": \"Eo Charging\", \"website\": \"https://www.eocharging.com/\"},\n",
        "    {\"id\": 65212, \"name\": \"Prewave\", \"website\": \"https://www.prewave.com/\"},\n",
        "    {\"id\": 18533, \"name\": \"Viriciti\", \"website\": \"https://www.chargepoint.com/\"},\n",
        "    {\"id\": 2805, \"name\": \"EasyMile\", \"website\": \"https://www.easymile.com/\"},\n",
        "    {\"id\": 101741, \"name\": \"Everstream\", \"website\": \"https://www.everstream.ai/\"},\n",
        "    {\"id\": 110133, \"name\": \"Altus Power\", \"website\": \"https://www.altuspower.com/\"},\n",
        "    {\"id\": 12605, \"name\": \"Charm Industrial\", \"website\": \"https://www.charmindustrial.com/\"},\n",
        "    {\"id\": 105894, \"name\": \"Isotropic Systems\", \"website\": \"https://www.all.space/\"},\n",
        "    {\"id\": 400, \"name\": \"Caban Systems\", \"website\": \"https://www.cabanenergy.com/\"},\n",
        "    {\"id\": 34204, \"name\": \"BioBTX\", \"website\": \"https://biobtx.com/\"},\n",
        "    {\"id\": 6134, \"name\": \"Hydrogenious LOHC\", \"website\": \"https://hydrogenious.net/\"},\n",
        "    {\"id\": 12008, \"name\": \"Iogen\", \"website\": \"https://www.iogen.com/\"},\n",
        "    {\"id\": 6997, \"name\": \"Infinited Fiber Company\", \"website\": \"https://www.infinitedfiber.com/\"}\n",
        "]\n",
        "\n",
        "# Function to call GPT-4 to assist in parsing and summarizing content\n",
        "def call_gpt4(prompt):\n",
        "\n",
        "  response = openai.Completion.create(\n",
        "    engine=\"gpt-3.5-turbo\",\n",
        "    prompt=prompt,\n",
        "    max_tokens=150,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "\n",
        "  return response.choices[0].text.strip()\n",
        "\n",
        "# Function to scrape and use GPT-4 for processing\n",
        "def scrape_company_data(company):\n",
        "    try:\n",
        "        response = requests.get(company['website'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Generate prompts for GPT-4\n",
        "        description_prompt = f\"Extract and summarize the company description from this HTML:\\n\\n{soup.prettify()}\"\n",
        "        hq_offices_prompt = f\"Extract the headquarters and office locations from this HTML:\\n\\n{soup.prettify()}\"\n",
        "        clients_prompt = f\"List the clients of the company from this HTML:\\n\\n{soup.prettify()}\"\n",
        "        news_prompt = f\"Summarize the latest news articles from this HTML:\\n\\n{soup.prettify()}\"\n",
        "\n",
        "        # Call GPT-4\n",
        "        description = call_gpt4(description_prompt)\n",
        "        hq_offices = call_gpt4(hq_offices_prompt)\n",
        "        clients = call_gpt4(clients_prompt)\n",
        "        news = call_gpt4(news_prompt)\n",
        "\n",
        "        return {\n",
        "            \"Company ID\": company['id'],\n",
        "            \"Company Name\": company['name'],\n",
        "            \"Website\": company['website'],\n",
        "            \"Description\": description,\n",
        "            \"HQ and Offices\": hq_offices,\n",
        "            \"Clients\": clients,\n",
        "            \"News\": news\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {company['name']}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Scrape data for all companies\n",
        "data = []\n",
        "for company in companies:\n",
        "    company_data = scrape_company_data(company)\n",
        "    if company_data:\n",
        "        data.append(company_data)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save DataFrame to Excel mimicking SQL table structure\n",
        "df.to_excel('company_data_with_gpt4.xlsx', index=False)\n",
        "\n",
        "print(\"Data scraping with GPT-4 complete. The results have been saved to 'company_data_with_gpt4.xlsx'.\")\n"
      ],
      "metadata": {
        "id": "w52LSBQ6BSYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}